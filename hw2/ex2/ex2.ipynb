{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyON326pZu29qzaiw39LCPH4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"asUdbGJXhyUA","executionInfo":{"status":"ok","timestamp":1701017294042,"user_tz":-60,"elapsed":4,"user":{"displayName":"Petrea Calin","userId":"11388743993003251972"}},"outputId":"5074beb1-318a-4c9d-c105-26fe8de1ece5"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5IsMUeKSiJo7","executionInfo":{"status":"ok","timestamp":1701017401779,"user_tz":-60,"elapsed":246,"user":{"displayName":"Petrea Calin","userId":"11388743993003251972"}},"outputId":"d23cf89f-0fab-4710-ed2a-c3b2d9484f85"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Nov 26 16:50:01 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["%%writefile lab2_ex2.cu\n","#include <stdio.h>\n","#include <sys/time.h>\n","\n","#define TPB 16\n","#define DataType double\n","#define DOUBLE_MIN -5\n","#define DOUBLE_MAX 5\n","\n","#define CUDA_CHECK(call) \\\n","    do { \\\n","        cudaError_t err = call; \\\n","        if (err != cudaSuccess) { \\\n","            fprintf(stderr, \"CUDA error: %s, line: %d\\n\", cudaGetErrorString(err), __LINE__); \\\n","            exit(EXIT_FAILURE); \\\n","        } \\\n","    } while(0)\n","\n","// Compute C = A * B\n","__global__ void gemm(DataType *A, DataType *B, DataType *C, int numARows,\n","                    int numAColumns, int numBRows, int numBColumns){\n","    //@@ Insert code to implement matrix multiplication here\n","    int index_x = blockIdx.x * blockDim.x + threadIdx.x;\n","    int index_y = blockIdx.y * blockDim.y + threadIdx.y;\n","\n","    if (index_y < numARows && index_x < numBColumns)\n","    {\n","        DataType sum = 0;\n","        for (int k = 0; k < numAColumns; k++){\n","            sum += A[index_y * numAColumns + k] * B[k * numBColumns + index_x];\n","        }\n","        C[index_y * numBColumns + index_x] = sum;\n","    }\n","}\n","\n","//@@ Insert code to implement timer start\n","void timerStart(struct timeval *start) {\n","    gettimeofday(start, NULL);\n","}\n","\n","//@@ Insert code to implement timer stop\n","double timerStop(struct timeval *start) {\n","    struct timeval end;\n","    gettimeofday(&end, NULL);\n","    double time = (end.tv_sec - start->tv_sec) * 1000.0;\n","    time += (end.tv_usec - start->tv_usec) / 1000.0;\n","    return time;\n","}\n","\n","double randDouble(double min, double max) {\n","    double scale = rand() / (double)RAND_MAX;\n","    return min + scale * (max-min);\n","}\n","\n","int main(int argc, char **argv) {\n","    DataType *hostA; // The A matrix\n","    DataType *hostB; // The B matrix\n","    DataType *hostC; // The output C matrix\n","    DataType *resultRef; // The reference result\n","    DataType *deviceA;\n","    DataType *deviceB;\n","    DataType *deviceC;\n","    int numARows;    // number of rows in the matrix A\n","    int numAColumns; // number of columns in the matrix A\n","    int numBRows;    // number of rows in the matrix B\n","    int numBColumns; // number of columns in the matrix B\n","    int numCRows;\n","    int numCColumns;\n","\n","    struct timeval copyToDevice, copyFromDevice, kernelExecution;\n","    double copyToDeviceTime, copyFromDeviceTime, kernelExecutionTime;\n","\n","    //@@ Insert code below to read in numARows, numAColumns, numBColumns from args\n","    if (argc >= 4)\n","    {\n","        numARows = atoi(argv[1]);\n","        numAColumns = atoi(argv[2]);\n","        numBRows = numAColumns;\n","        numBColumns = atoi(argv[3]);\n","        numCRows = numARows;\n","        numCColumns = numBColumns;\n","    }\n","    printf(\"Input matrix dim (%d x %d) (%d x %d) (%d x %d)\\n\", numARows, numAColumns, numBRows, numBColumns, numCRows, numCColumns);\n","\n","    //@@ Insert code below to allocate Host memory for input and output\n","    hostA = (DataType *)malloc(numARows * numAColumns * sizeof(DataType));\n","    hostB = (DataType *)malloc(numBRows * numBColumns * sizeof(DataType));\n","    hostC = (DataType *)malloc(numCRows * numCColumns * sizeof(DataType));\n","    resultRef = (DataType *)malloc(numCRows * numCColumns * sizeof(DataType));\n","\n","    //@@ Insert code below to initialize hostA and hostB to random numbers, and create reference result in CPU\n","    //printf(\"\\n\\nhostA:\\n\");\n","    for(int i = 0; i < numARows; i++){\n","        for(int j = 0; j < numAColumns; j++){\n","            hostA[i * numAColumns + j] = randDouble(DOUBLE_MIN, DOUBLE_MAX);\n","            //printf(\"%.3f \", hostA[i * numAColumns + j]);\n","        }\n","        //printf(\"\\n\");\n","    }\n","\n","    //printf(\"\\n\\nhostB:\\n\");\n","    for(int i = 0; i < numBRows; i++){\n","        for(int j = 0; j < numBColumns; j++){\n","            hostB[i * numBColumns + j] = randDouble(DOUBLE_MIN, DOUBLE_MAX);\n","            //printf(\"%.3f \", hostB[i * numBColumns + j]);\n","        }\n","        //printf(\"\\n\");\n","    }\n","\n","    //printf(\"\\n\\nresultRef:\\n\");\n","    for(int i = 0; i < numCRows; i++){\n","        for(int j = 0; j < numCColumns; j++){\n","            resultRef[i * numCColumns + j] = 0;\n","\n","            for (int k = 0; k < numAColumns; k++) {\n","                resultRef[i * numCColumns + j] += hostA[i * numAColumns + k] * hostB[k * numBColumns + j];\n","            }\n","            //printf(\"%.3f \", resultRef[i * numCColumns + j]);\n","        }\n","        //printf(\"\\n\");\n","    }\n","\n","\n","    //@@ Insert code below to allocate GPU memory here\n","    CUDA_CHECK(cudaMalloc((void **)&deviceA, numARows * numAColumns * sizeof(DataType)));\n","    CUDA_CHECK(cudaMalloc((void **)&deviceB, numBRows * numBColumns * sizeof(DataType)));\n","    CUDA_CHECK(cudaMalloc((void **)&deviceC, numCRows * numCColumns * sizeof(DataType)));\n","\n","\n","    //@@ Insert code to below to Copy memory to the GPU here\n","    timerStart(&copyToDevice);\n","    CUDA_CHECK(cudaMemcpy(deviceA, hostA, numARows * numAColumns * sizeof(DataType), cudaMemcpyHostToDevice));\n","    CUDA_CHECK(cudaMemcpy(deviceB, hostB, numBRows * numBColumns * sizeof(DataType), cudaMemcpyHostToDevice));\n","    copyToDeviceTime = timerStop(&copyToDevice);\n","\n","    //@@ Initialize the grid and block dimensions here\n","    dim3 DimGrid((numCColumns + TPB - 1) / TPB, (numCRows + TPB - 1) / TPB, 1);\n","    dim3 DimBlock(TPB, TPB, 1);\n","\n","    //@@ Launch the GPU Kernel here\n","    timerStart(&kernelExecution);\n","    gemm<<<DimGrid, DimBlock>>>(deviceA, deviceB, deviceC, numARows, numAColumns, numBRows, numCColumns);\n","    cudaDeviceSynchronize();\n","    kernelExecutionTime = timerStop(&kernelExecution);\n","\n","    //@@ Copy the GPU memory back to the CPU here\n","    timerStart(&copyFromDevice);\n","    CUDA_CHECK(cudaMemcpy(hostC, deviceC, numCRows * numCColumns * sizeof(DataType), cudaMemcpyDeviceToHost));\n","    copyFromDeviceTime = timerStop(&copyFromDevice);\n","\n","    //@@ Insert code below to compare the output with the reference\n","    double diff = 0.0;\n","    //printf(\"\\n\\nhostC:\\n\");\n","    for(int i = 0; i < numCRows; i++){\n","        for(int j = 0; j < numCColumns; j++) {\n","            diff += abs(hostC[i * numCColumns + j] - resultRef[i * numCColumns + j]);\n","            //printf(\"%.3f \", hostC[i * numCColumns + j]);\n","        }\n","        //printf(\"\\n\");\n","    }\n","\n","    printf(\"Average difference: %f\\n\\n\", diff/(double)(numCRows * numCColumns));\n","\n","    printf(\"\\nCopy to Device Time: %f ms\\n\", copyToDeviceTime);\n","    printf(\"Kernel Execution Time: %f ms\\n\", kernelExecutionTime);\n","    printf(\"Copy from Device Time: %f ms\\n\", copyFromDeviceTime);\n","\n","    //@@ Free the GPU memory here\n","    cudaFree((void *) deviceA);\n","    cudaFree((void *) deviceB);\n","    cudaFree((void *) deviceC);\n","\n","    //@@ Free the CPU memory here\n","    free(hostA);\n","    free(hostB);\n","    free(hostC);\n","\n","    return 0;\n","}\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AdHed7aQiQr4","executionInfo":{"status":"ok","timestamp":1701030057625,"user_tz":-60,"elapsed":357,"user":{"displayName":"Petrea Calin","userId":"11388743993003251972"}},"outputId":"d71054a3-5131-4429-e9dd-1d1e2efa5b90"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting lab2_ex2.cu\n"]}]},{"cell_type":"code","source":["!nvcc lab2_ex2.cu\n","!ls\n","!./a.out 128 128 128"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GP5Efc6JipNT","executionInfo":{"status":"ok","timestamp":1701030063348,"user_tz":-60,"elapsed":2358,"user":{"displayName":"Petrea Calin","userId":"11388743993003251972"}},"outputId":"517b8d7c-3fb3-4bac-a9c3-0afc97464f54"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["a.out  lab2_ex2.cu  sample_data\n","Input matrix dim (128 x 128) (128 x 128) (128 x 128)\n","Average difference: 0.000000\n","\n","\n","Copy to Device Time: 1.259000 ms\n","Kernel Execution Time: 0.092000 ms\n","Copy from Device Time: 0.129000 ms\n"]}]},{"cell_type":"code","source":["!nvprof ./a.out 128 128 128"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AG5c7Sj3ui3X","executionInfo":{"status":"ok","timestamp":1701030082974,"user_tz":-60,"elapsed":821,"user":{"displayName":"Petrea Calin","userId":"11388743993003251972"}},"outputId":"31ae6fe4-4343-4285-884d-9a2ebd7c7aad"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Input matrix dim (128 x 128) (128 x 128) (128 x 128)\n","==1198== NVPROF is profiling process 1198, command: ./a.out 128 128 128\n","Average difference: 0.000000\n","\n","\n","Copy to Device Time: 0.116000 ms\n","Kernel Execution Time: 0.098000 ms\n","Copy from Device Time: 0.134000 ms\n","==1198== Profiling application: ./a.out 128 128 128\n","==1198== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   60.29%  64.575us         1  64.575us  64.575us  64.575us  gemm(double*, double*, double*, int, int, int, int)\n","                   28.08%  30.079us         2  15.039us  13.919us  16.160us  [CUDA memcpy HtoD]\n","                   11.62%  12.448us         1  12.448us  12.448us  12.448us  [CUDA memcpy DtoH]\n","      API calls:   99.24%  224.83ms         3  74.944ms  2.8230us  224.82ms  cudaMalloc\n","                    0.48%  1.0955ms         1  1.0955ms  1.0955ms  1.0955ms  cuDeviceGetPCIBusId\n","                    0.11%  244.25us         3  81.416us  54.616us  133.46us  cudaMemcpy\n","                    0.06%  144.99us         3  48.330us  4.9770us  127.46us  cudaFree\n","                    0.05%  119.98us       101  1.1870us     134ns  51.097us  cuDeviceGetAttribute\n","                    0.03%  67.856us         1  67.856us  67.856us  67.856us  cudaDeviceSynchronize\n","                    0.01%  27.431us         1  27.431us  27.431us  27.431us  cudaLaunchKernel\n","                    0.01%  24.790us         1  24.790us  24.790us  24.790us  cuDeviceGetName\n","                    0.00%  1.7920us         3     597ns     202ns  1.3690us  cuDeviceGetCount\n","                    0.00%     993ns         2     496ns     239ns     754ns  cuDeviceGet\n","                    0.00%     505ns         1     505ns     505ns     505ns  cuModuleGetLoadingMode\n","                    0.00%     432ns         1     432ns     432ns     432ns  cuDeviceTotalMem\n","                    0.00%     226ns         1     226ns     226ns     226ns  cuDeviceGetUuid\n"]}]},{"cell_type":"code","source":["!ncu ./a.out 128 128 128"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kPh9y1U1upen","executionInfo":{"status":"ok","timestamp":1701030086761,"user_tz":-60,"elapsed":1752,"user":{"displayName":"Petrea Calin","userId":"11388743993003251972"}},"outputId":"b7301cb2-b6f6-448c-f743-84555f36e9e3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Input matrix dim (128 x 128) (128 x 128) (128 x 128)\n","==PROF== Connected to process 1230 (/content/a.out)\n","==PROF== Profiling \"gemm\" - 0: 0%....50%....100% - 8 passes\n","Average difference: 0.000000\n","\n","\n","Copy to Device Time: 0.114000 ms\n","Kernel Execution Time: 400.403000 ms\n","Copy from Device Time: 0.158000 ms\n","==PROF== Disconnected from process 1230\n","[1230] a.out@127.0.0.1\n","  gemm(double *, double *, double *, int, int, int, int), 2023-Nov-26 20:21:26, Context 1, Stream 7\n","    Section: GPU Speed Of Light Throughput\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    DRAM Frequency                                                           cycle/nsecond                           4.97\n","    SM Frequency                                                             cycle/usecond                         581.55\n","    Elapsed Cycles                                                                   cycle                         36,947\n","    Memory [%]                                                                           %                          17.95\n","    DRAM Throughput                                                                      %                           2.21\n","    Duration                                                                       usecond                          63.52\n","    L1/TEX Cache Throughput                                                              %                          35.80\n","    L2 Cache Throughput                                                                  %                           3.40\n","    SM Active Cycles                                                                 cycle                      28,676.38\n","    Compute (SM) [%]                                                                     %                          70.96\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    WRN   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   \n","          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      \n","          could be reduced or moved to look-up tables.                                                                  \n","\n","    Section: Launch Statistics\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    Block Size                                                                                                        256\n","    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n","    Grid Size                                                                                                          64\n","    Registers Per Thread                                                   register/thread                             64\n","    Shared Memory Configuration Size                                                 Kbyte                          32.77\n","    Driver Shared Memory Per Block                                              byte/block                              0\n","    Dynamic Shared Memory Per Block                                             byte/block                              0\n","    Static Shared Memory Per Block                                              byte/block                              0\n","    Threads                                                                         thread                         16,384\n","    Waves Per SM                                                                                                     0.40\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the \n","          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   \n","          hardware busy.                                                                                                \n","\n","    Section: Occupancy\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    Block Limit SM                                                                   block                             16\n","    Block Limit Registers                                                            block                              4\n","    Block Limit Shared Mem                                                           block                             16\n","    Block Limit Warps                                                                block                              4\n","    Theoretical Active Warps per SM                                                   warp                             32\n","    Theoretical Occupancy                                                                %                            100\n","    Achieved Occupancy                                                                   %                          42.83\n","    Achieved Active Warps Per SM                                                      warp                          13.70\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n","          theoretical (100.0%) and measured achieved occupancy (42.8%) can be the result of warp scheduling overheads   \n","          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n","          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n","          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n","          optimizing occupancy.                                                                                         \n","\n"]}]},{"cell_type":"code","source":["!ncu ./a.out 511 1023 4094"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CUlyNvy8o8EN","executionInfo":{"status":"ok","timestamp":1701030141907,"user_tz":-60,"elapsed":39032,"user":{"displayName":"Petrea Calin","userId":"11388743993003251972"}},"outputId":"03ca8ded-d180-4cae-d62e-198eeaee4be7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Input matrix dim (511 x 1023) (1023 x 4094) (511 x 4094)\n","==PROF== Connected to process 1323 (/content/a.out)\n","==PROF== Profiling \"gemm\" - 0: 0%....50%....100% - 8 passes\n","Average difference: 0.000000\n","\n","\n","Copy to Device Time: 8.724000 ms\n","Kernel Execution Time: 686.054000 ms\n","Copy from Device Time: 13.442000 ms\n","==PROF== Disconnected from process 1323\n","[1323] a.out@127.0.0.1\n","  gemm(double *, double *, double *, int, int, int, int), 2023-Nov-26 20:22:21, Context 1, Stream 7\n","    Section: GPU Speed Of Light Throughput\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    DRAM Frequency                                                           cycle/nsecond                           5.00\n","    SM Frequency                                                             cycle/usecond                         585.01\n","    Elapsed Cycles                                                                   cycle                     27,876,330\n","    Memory [%]                                                                           %                          25.07\n","    DRAM Throughput                                                                      %                          11.46\n","    Duration                                                                       msecond                          47.65\n","    L1/TEX Cache Throughput                                                              %                          50.14\n","    L2 Cache Throughput                                                                  %                           5.70\n","    SM Active Cycles                                                                 cycle                  27,815,991.65\n","    Compute (SM) [%]                                                                     %                          96.20\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n","          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n","          Start by analyzing workloads in the Compute Workload Analysis section.                                        \n","\n","    Section: Launch Statistics\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    Block Size                                                                                                        256\n","    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n","    Grid Size                                                                                                       8,192\n","    Registers Per Thread                                                   register/thread                             64\n","    Shared Memory Configuration Size                                                 Kbyte                          32.77\n","    Driver Shared Memory Per Block                                              byte/block                              0\n","    Dynamic Shared Memory Per Block                                             byte/block                              0\n","    Static Shared Memory Per Block                                              byte/block                              0\n","    Threads                                                                         thread                      2,097,152\n","    Waves Per SM                                                                                                    51.20\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","\n","    Section: Occupancy\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    Block Limit SM                                                                   block                             16\n","    Block Limit Registers                                                            block                              4\n","    Block Limit Shared Mem                                                           block                             16\n","    Block Limit Warps                                                                block                              4\n","    Theoretical Active Warps per SM                                                   warp                             32\n","    Theoretical Occupancy                                                                %                            100\n","    Achieved Occupancy                                                                   %                          99.17\n","    Achieved Active Warps Per SM                                                      warp                          31.73\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    INF   This kernel's theoretical occupancy is not impacted by any block limit.                                       \n","\n"]}]}]}